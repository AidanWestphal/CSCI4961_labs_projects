{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77fdbde9-c4a7-4f55-9c35-1b1ba4a97e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from time import time\n",
    "import hashlib\n",
    "import psutil\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import rustworkx as rx\n",
    "from rustworkx.visualization import mpl_draw as draw_graph\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from qiskit import QuantumCircuit, transpile, assemble\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.circuit.library import QAOAAnsatz\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit_ibm_runtime import Session, EstimatorV2 as Estimator\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "from qiskit.providers.fake_provider import GenericBackendV2\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm, SAGPooling\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ce0d5-4bbd-4bfa-9f5e-29685c2dda14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98547d10-9c15-4124-9183-2757aa1a439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_cut(bitstring, graph):\n",
    "    cut_value = 0\n",
    "    for (u, v) in graph.edges():\n",
    "        if bitstring[u] != bitstring[v]:  # Different partitions\n",
    "            cut_value += graph[u][v]['weight']\n",
    "    return cut_value\n",
    "\n",
    "# Function to create QAOA circuit\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "def create_qaoa_circuit(gamma, beta, graph, depth):\n",
    "    num_qubits = len(graph.nodes)\n",
    "    qc = QuantumCircuit(num_qubits, num_qubits)  # Allocate classical bits\n",
    "\n",
    "    # Apply Hadamard gates to all qubits\n",
    "    qc.h(range(num_qubits))\n",
    "    qc.barrier()\n",
    "\n",
    "    for _ in range(depth):\n",
    "        # Apply the problem unitary (Cost Hamiltonian)\n",
    "        for (u, v) in graph.edges:\n",
    "            qc.cx(u, v)\n",
    "            qc.rz(2 * gamma * graph[u][v]['weight'], v)\n",
    "            qc.cx(u, v)\n",
    "        qc.barrier()\n",
    "\n",
    "        # Apply the mixer unitary (Mixer Hamiltonian)\n",
    "        for qubit in range(num_qubits):\n",
    "            qc.rx(2 * beta, qubit)\n",
    "        qc.barrier()\n",
    "\n",
    "    # Measurement\n",
    "    qc.measure(range(num_qubits), range(num_qubits))\n",
    "\n",
    "    return qc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a61d4dc5-3723-4f65-a8b2-513dd9323b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAOA(\n",
    "    beta: float,\n",
    "    gamma: float,\n",
    "    fakeBackend: bool,\n",
    "    depth: int,\n",
    "    adjacencyList: list[list[int]],\n",
    ") -> dict:\n",
    "    if fakeBackend:\n",
    "        backend = GenericBackendV2(len(adjacencyList))\n",
    "    else:\n",
    "        backend = rensslearBackend  # Define your actual backend here\n",
    "    qc = QAOA_circuit(beta, gamma, depth, adjacencyList)\n",
    "    job = backend.run(qc, shots=1024)\n",
    "    result = job.result()\n",
    "    return result.get_counts()\n",
    "\n",
    "\n",
    "# Takes params->[beta, gamma], a bool representing if the backend is fake, depth of the circuit, and adjacnecy list;\n",
    "# returns the average cut of the given graph\n",
    "def cost_function(params, fakeBackend: bool, depth: int, adj_list: list[list[int]]):\n",
    "    beta, gamma = params\n",
    "    counts = QAOA(beta, gamma, fakeBackend, depth, adj_list)\n",
    "    # Calculate the expectation value of the cost Hamiltonian\n",
    "    cost = 0\n",
    "    for bitstring, count in counts.items():\n",
    "        bit_val = [int(bit) for bit in bitstring]\n",
    "        cut_value = 0\n",
    "        for i in range(len(adj_list)):\n",
    "            for j in adj_list[i]:\n",
    "                if i < j:\n",
    "                    cut_value += bit_val[i] != bit_val[j]\n",
    "        cost += cut_value * count\n",
    "    return cost / 1024\n",
    "def convert_to_graph_dict(G):\n",
    "    graph_dict = {}\n",
    "\n",
    "    for u, neighbors in enumerate(G):\n",
    "        for v in neighbors:\n",
    "            if (v, u) not in graph_dict:  # Avoid duplicate edges for undirected graph\n",
    "                graph_dict[(u, v)] = 1.0\n",
    "\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c158c552-ed6e-486a-b1d9-cb490af7bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a beta, gamma, depth, and adjacencyList; returns the QAOA circuit\n",
    "def QAOA_circuit(beta: float, gamma: float, depth: int, adjacencyList: list[list[int]]):\n",
    "    num_qubits = len(adjacencyList)\n",
    "    qc = QuantumCircuit(num_qubits, num_qubits)  # Allocate classical bits\n",
    "\n",
    "    # Initial Hadamard gates\n",
    "    qc.h(range(num_qubits))\n",
    "    qc.barrier()\n",
    "\n",
    "    for _ in range(depth):\n",
    "        # Apply the Hamiltonian cost function\n",
    "        for edgeIndex, neighbors in enumerate(adjacencyList):\n",
    "            for edge in neighbors:\n",
    "                if edge > edgeIndex:\n",
    "                    qc.cz(edgeIndex, edge)\n",
    "                    qc.rz(2 * gamma, edgeIndex)\n",
    "                    qc.cz(edge, edgeIndex)\n",
    "        qc.barrier()\n",
    "\n",
    "        # Apply RX gates to each qubit for the mixer Hamiltonian\n",
    "        for qubit in range(num_qubits):\n",
    "            qc.rx(2 * beta, qubit)\n",
    "        qc.barrier()\n",
    "\n",
    "    # Measurement\n",
    "    qc.measure(range(num_qubits), range(num_qubits))\n",
    "\n",
    "    return qc\n",
    "\n",
    "    # Takes a beta, gamma, bool representing if the backend is fake, depth of the circuit, and adjacnecy list; returns a dict of the bitstrings produced\n",
    "def QAOA(\n",
    "    beta: float,\n",
    "    gamma: float,\n",
    "    fakeBackend: bool,\n",
    "    depth: int,\n",
    "    adjacencyList: list[list[int]],\n",
    ") -> dict:\n",
    "    if fakeBackend:\n",
    "        backend = GenericBackendV2(len(adjacencyList))\n",
    "    else:\n",
    "        backend = rensslearBackend  # Define your actual backend here\n",
    "    qc = QAOA_circuit(beta, gamma, depth, adjacencyList)\n",
    "    job = backend.run(qc, shots=1024)\n",
    "    result = job.result()\n",
    "    return result.get_counts()\n",
    "\n",
    "\n",
    "# Takes params->[beta, gamma], a bool representing if the backend is fake, depth of the circuit, and adjacnecy list;\n",
    "# returns the average cut of the given graph\n",
    "def cost_function(params, fakeBackend: bool, depth: int, adj_list: list[list[int]]):\n",
    "    beta, gamma = params\n",
    "    counts = QAOA(beta, gamma, fakeBackend, depth, adj_list)\n",
    "    # Calculate the expectation value of the cost Hamiltonian\n",
    "    cost = 0\n",
    "    for bitstring, count in counts.items():\n",
    "        bit_val = [int(bit) for bit in bitstring]\n",
    "        cut_value = 0\n",
    "        for i in range(len(adj_list)):\n",
    "            for j in adj_list[i]:\n",
    "                if i < j:\n",
    "                    cut_value += bit_val[i] != bit_val[j]\n",
    "        cost += cut_value * count\n",
    "    return cost / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab7aa5cb-b3fa-4448-ac91-dda5fc087905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    with open(filename, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a934b9f-0d52-4dbc-bd59-487c6552143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(adj_list, num_nodes):\n",
    "    adj_matrix = [[0] * num_nodes for _ in range(num_nodes)]  # Initialize with zeros\n",
    "    for node, neighbors in enumerate(adj_list):\n",
    "        for neighbor in neighbors:\n",
    "            adj_matrix[node][neighbor] = 1\n",
    "            adj_matrix[neighbor][node] = 1  # Assuming undirected graph\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5ec101b-9098-4fbd-b945-914454987b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix_to_list(adj_matrix):\n",
    "    adjacency_list = []\n",
    "    num_nodes = len(adj_matrix)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        neighbors = []\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i][j] != 0:\n",
    "                neighbors.append(j)\n",
    "        adjacency_list.append(neighbors)\n",
    "\n",
    "    return adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "748a5508-2085-45bf-8903-f668ea1c131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTorchObject(graph):\n",
    "    return torch.tensor(graph)\n",
    "    \n",
    "class DataObject:\n",
    "    def __init__(self, gamma, beta, graph):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.graph = graph\n",
    "        self.torchObject = createTorchObject(graph)\n",
    "        \n",
    "    def getGamma(self):\n",
    "        return self.gamma\n",
    "        \n",
    "    def getBeta(self):\n",
    "        return self.beta\n",
    "        \n",
    "    def getGraph(self):\n",
    "        return self.graph       \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, dataObjects):\n",
    "        self.list_IDs = list_IDs # list\n",
    "        self.dataObjects = dataObjects # dictionary\n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.list_IDs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        dataObject = self.dataObjects[ID]\n",
    "        X = dataObject.torchObject \n",
    "        Y = [dataObject.getGamma(), dataObject.getBeta()]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06721e20-3492-4548-a45d-4dcd91dcf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_csv_parser(filename, skip):\n",
    "  data = []\n",
    "  IDCounter = 0\n",
    "  list_IDs = []\n",
    "  dataObjects = dict()\n",
    "  with open(filename, mode='r', newline='') as file:\n",
    "      # Make an array like this: [[graphid, adjlist], [graphid, adjlist], ...]\n",
    "      reader = csv.reader(file)\n",
    "      max_length = 0\n",
    "      for row in reader:\n",
    "          data.append(row)\n",
    "      # Remove the header\n",
    "      data = data[1:]\n",
    "      new_data = []\n",
    "      for row in data:\n",
    "        if type(eval(row[1])) != list:\n",
    "          continue\n",
    "        new_data.append(row)\n",
    "        if len(eval(row[1])) > max_length:\n",
    "          max_length = len(eval(row[1]))\n",
    "      data = new_data\n",
    "      # Convert the graph[n][1] from a string into an adjacency list\n",
    "      for i in range(skip, len(data)):\n",
    "          currentId = 'id-' + str(IDCounter)\n",
    "          list_IDs.append(currentId)\n",
    "          beta = float(data[i][2])\n",
    "          gamma = float(data[i][3])\n",
    "          graph = eval(data[i][1])\n",
    "          matrix = create_adjacency_matrix(graph, max_length)\n",
    "          dataObjects[currentId] = DataObject(gamma, beta, matrix)\n",
    "          IDCounter += 1\n",
    "  print(\"length: \", IDCounter)\n",
    "  return list_IDs, dataObjects, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29fba6ca-b8de-4c1b-9e07-9177d499a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  498\n"
     ]
    }
   ],
   "source": [
    "# fullDataset = Dataset(list_IDs, dataObjects)\n",
    "# Read graphs from csv\n",
    "num = 3\n",
    "filename = ''\n",
    "if num == 1:\n",
    "    filename = './allCSVFiles-6.csv'\n",
    "elif num == 2:\n",
    "    filename = \"./allCSVFiles-3.csv\"\n",
    "elif num == 3:\n",
    "    filename = \"dataset-1000.csv\"\n",
    "    \n",
    "# convert to list of lists\n",
    "list_IDs, dataObjects, max_length = all_csv_parser(filename, 500)\n",
    "\n",
    "# convert to classes\n",
    "train_IDs, test_IDs = train_test_split(list_IDs, test_size=0.3, random_state=42)\n",
    "val_IDs, test_IDs = train_test_split(test_IDs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets split the data\n",
    "train_dataset = Dataset(train_IDs, dataObjects)\n",
    "val_dataset = Dataset(val_IDs, dataObjects)\n",
    "test_dataset = Dataset(test_IDs, dataObjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1251c3ea-9a55-4d2b-a4cf-85eeae3863df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len((dataObjects['id-0'].getGraph()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eba23046-b561-465a-bee1-758c49c28417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come back to this should the gamma and beta be tensors?\n",
    "# should the x object be returned back with the number one for model creation?\n",
    "# print(fullDataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dfbd821-de57-4185-a2e6-03b910d7457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(actual_params, predicted_params, graphs, batch_size):\n",
    "    if len(actual_params) < batch_size:\n",
    "        print(f\"Skipping batch as its size {len(actual_params)} is smaller than {batch_size}.\")\n",
    "        return torch.tensor(0.0, device='cuda', requires_grad=True)\n",
    "    \n",
    "    randomIndicies = random.sample(range(0, batch_size), 5)\n",
    "    totalLoss = 0\n",
    "    for i in randomIndicies:\n",
    "        gammaPredicted = predicted_params[0][i].item()\n",
    "        betaPredicted = predicted_params[1][i].item()\n",
    "        gammaKnown = actual_params[i][0].item()\n",
    "        betaKnow = actual_params[i][1].item()\n",
    "        graph = graphs[i].tolist()\n",
    "        expected_cost = cost_function([betaKnow, gammaKnown], True, 1, graph)\n",
    "        predicted_cost = cost_function([betaPredicted, gammaPredicted], True, 1, graph)\n",
    "        totalLoss += (predicted_cost - expected_cost)\n",
    "    #print(\"predicted\", predicted_params)\n",
    "    print(\"total Loss: \", totalLoss)\n",
    "    totalLoss += 10\n",
    "    if totalLoss < 0:\n",
    "        totalLoss = 0.0\n",
    "    return torch.tensor(totalLoss, device='cuda', requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab32d8e1-b665-4fb0-9d6c-c4593af52d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_edge_index(adj):\n",
    "    edge_index = adj.nonzero(as_tuple=True)\n",
    "    edge_index = torch.stack(edge_index, dim=0)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3844840-eeea-4ab9-94e9-a41816edc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNRegressor, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels=1, out_channels=128)\n",
    "        self.conv2 = GCNConv(in_channels=128, out_channels=256)\n",
    "        self.conv3 = GCNConv(in_channels=256, out_channels=256)\n",
    "        self.conv4 = GCNConv(in_channels=256, out_channels=512)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Predicting gamma and beta\n",
    "\n",
    "    def forward(self, batch):\n",
    "        adj_matrices, params = batch\n",
    "        batch_size, num_nodes, _ = adj_matrices.size()\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            adj_matrix = adj_matrices[i]\n",
    "            edge_index = adj_to_edge_index(adj_matrix)\n",
    "            node_features = torch.ones((num_nodes, 1)).to(device)  # Example with single feature\n",
    "            \n",
    "            x = F.relu(self.bn1(self.conv1(node_features, edge_index)))\n",
    "            x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "            x = F.relu(self.bn3(self.conv3(x, edge_index)))\n",
    "            x = F.relu(self.bn4(self.conv4(x, edge_index)))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            out = self.fc3(x.mean(dim=0))  # Assuming global pooling and reducing to (batch_size, 2)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        return torch.stack(outputs)\n",
    "    #def forward(self, x):\n",
    "      #  x = self.relu(self.conv1(x))\n",
    "     #   x = self.pool(x)\n",
    "      #  x = self.relu(self.conv2(x))\n",
    "     #   x = self.pool(x)\n",
    "     #   x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "      #  x = self.relu(self.fc1(x))\n",
    "      #  x = self.fc2(x)\n",
    "      #  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c993967d-600f-4a6c-a139-f1660d0ded8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    print(\"not using GPU\")\n",
    "# might need to change to use multiple gpus\n",
    "device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# load the data\n",
    "current_batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=current_batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=current_batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "model = GNNRegressor().to(device)\n",
    "max_epochs = 20\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09230e55-947a-4974-83e8-6dc96dce8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Loss:  -0.755859375\n",
      "total Loss:  -0.22265625\n",
      "total Loss:  -0.0029296875\n",
      "total Loss:  0.0732421875\n",
      "total Loss:  0.40234375\n",
      "total Loss:  -0.5009765625\n",
      "total Loss:  0.4150390625\n",
      "total Loss:  -0.0947265625\n",
      "total Loss:  -0.29296875\n",
      "total Loss:  -0.080078125\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  0\n",
      "Time ellapsed:  -754.1124737262726\n",
      "total Loss:  -0.3662109375\n",
      "total Loss:  0.29296875\n",
      "total Loss:  -0.611328125\n",
      "total Loss:  -1.2421875\n",
      "total Loss:  0.1201171875\n",
      "total Loss:  0.2373046875\n",
      "total Loss:  -0.40625\n",
      "total Loss:  -1.4833984375\n",
      "total Loss:  -0.0654296875\n",
      "total Loss:  -0.1337890625\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  1\n",
      "Time ellapsed:  -724.1959979534149\n",
      "total Loss:  -0.017578125\n",
      "total Loss:  -0.4580078125\n",
      "total Loss:  -0.2890625\n",
      "total Loss:  -0.3818359375\n",
      "total Loss:  0.224609375\n",
      "total Loss:  -0.1259765625\n",
      "total Loss:  -0.021484375\n",
      "total Loss:  0.193359375\n",
      "total Loss:  0.56640625\n",
      "total Loss:  -0.3603515625\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  2\n",
      "Time ellapsed:  -753.6206393241882\n",
      "total Loss:  0.5302734375\n",
      "total Loss:  0.9365234375\n",
      "total Loss:  0.25\n",
      "total Loss:  -0.419921875\n",
      "total Loss:  -1.0751953125\n",
      "total Loss:  -0.0693359375\n",
      "total Loss:  -0.181640625\n",
      "total Loss:  -0.4267578125\n",
      "total Loss:  0.0029296875\n",
      "total Loss:  0.3984375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  3\n",
      "Time ellapsed:  -689.1597435474396\n",
      "total Loss:  0.390625\n",
      "total Loss:  0.22265625\n",
      "total Loss:  -0.08984375\n",
      "total Loss:  0.267578125\n",
      "total Loss:  1.0\n",
      "total Loss:  -0.291015625\n",
      "total Loss:  0.0732421875\n",
      "total Loss:  0.482421875\n",
      "total Loss:  0.3720703125\n",
      "total Loss:  -0.3251953125\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  4\n",
      "Time ellapsed:  -718.3692798614502\n",
      "total Loss:  -0.078125\n",
      "total Loss:  -0.78515625\n",
      "total Loss:  -0.564453125\n",
      "total Loss:  -0.4267578125\n",
      "total Loss:  -0.064453125\n",
      "total Loss:  0.41796875\n",
      "total Loss:  0.2138671875\n",
      "total Loss:  -0.078125\n",
      "total Loss:  -0.6943359375\n",
      "total Loss:  -0.248046875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  5\n",
      "Time ellapsed:  -691.2244534492493\n",
      "total Loss:  -0.251953125\n",
      "total Loss:  0.359375\n",
      "total Loss:  0.474609375\n",
      "total Loss:  0.587890625\n",
      "total Loss:  -0.4609375\n",
      "total Loss:  -0.59765625\n",
      "total Loss:  -0.837890625\n",
      "total Loss:  -0.37109375\n",
      "total Loss:  0.4033203125\n",
      "total Loss:  -0.09375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  6\n",
      "Time ellapsed:  -747.1654849052429\n",
      "total Loss:  0.0986328125\n",
      "total Loss:  0.509765625\n",
      "total Loss:  -0.0810546875\n",
      "total Loss:  0.7294921875\n",
      "total Loss:  -0.333984375\n",
      "total Loss:  -0.34765625\n",
      "total Loss:  -0.0107421875\n",
      "total Loss:  -0.7392578125\n",
      "total Loss:  0.740234375\n",
      "total Loss:  0.2275390625\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  7\n",
      "Time ellapsed:  -731.3177945613861\n",
      "total Loss:  -0.0185546875\n",
      "total Loss:  0.322265625\n",
      "total Loss:  0.27734375\n",
      "total Loss:  0.0478515625\n",
      "total Loss:  -0.0263671875\n",
      "total Loss:  -0.5244140625\n",
      "total Loss:  0.056640625\n",
      "total Loss:  0.1474609375\n",
      "total Loss:  0.20703125\n",
      "total Loss:  0.171875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  8\n",
      "Time ellapsed:  -681.6847732067108\n",
      "total Loss:  -0.35546875\n",
      "total Loss:  -0.599609375\n",
      "total Loss:  0.3251953125\n",
      "total Loss:  0.6982421875\n",
      "total Loss:  -0.279296875\n",
      "total Loss:  0.2451171875\n",
      "total Loss:  -0.1826171875\n",
      "total Loss:  -0.078125\n",
      "total Loss:  -0.607421875\n",
      "total Loss:  0.1201171875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  9\n",
      "Time ellapsed:  -710.0826768875122\n",
      "total Loss:  -0.740234375\n",
      "total Loss:  -0.3896484375\n",
      "total Loss:  0.630859375\n",
      "total Loss:  0.2177734375\n",
      "total Loss:  0.8271484375\n",
      "total Loss:  -0.03125\n",
      "total Loss:  0.0263671875\n",
      "total Loss:  0.55859375\n",
      "total Loss:  0.4736328125\n",
      "total Loss:  0.3779296875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  10\n",
      "Time ellapsed:  -711.2290484905243\n",
      "total Loss:  0.05078125\n",
      "total Loss:  -0.0712890625\n",
      "total Loss:  0.236328125\n",
      "total Loss:  -0.5029296875\n",
      "total Loss:  0.126953125\n",
      "total Loss:  -0.44140625\n",
      "total Loss:  0.744140625\n",
      "total Loss:  -0.0009765625\n",
      "total Loss:  -0.1484375\n",
      "total Loss:  0.2099609375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  11\n",
      "Time ellapsed:  -667.7358901500702\n",
      "total Loss:  -0.583984375\n",
      "total Loss:  -0.16796875\n",
      "total Loss:  -0.0263671875\n",
      "total Loss:  -0.248046875\n",
      "total Loss:  -0.751953125\n",
      "total Loss:  -0.892578125\n",
      "total Loss:  0.2421875\n",
      "total Loss:  0.1533203125\n",
      "total Loss:  -0.0380859375\n",
      "total Loss:  -0.875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  12\n",
      "Time ellapsed:  -698.0018215179443\n",
      "total Loss:  -0.60546875\n",
      "total Loss:  0.107421875\n",
      "total Loss:  0.0341796875\n",
      "total Loss:  0.4443359375\n",
      "total Loss:  -0.2255859375\n",
      "total Loss:  -0.326171875\n",
      "total Loss:  -0.9912109375\n",
      "total Loss:  0.38671875\n",
      "total Loss:  -0.1865234375\n",
      "total Loss:  -0.2294921875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  13\n",
      "Time ellapsed:  -706.8770594596863\n",
      "total Loss:  0.33203125\n",
      "total Loss:  0.5654296875\n",
      "total Loss:  0.443359375\n",
      "total Loss:  0.1103515625\n",
      "total Loss:  0.0419921875\n",
      "total Loss:  0.1689453125\n",
      "total Loss:  -0.087890625\n",
      "total Loss:  0.3564453125\n",
      "total Loss:  -0.6767578125\n",
      "total Loss:  -0.5146484375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  14\n",
      "Time ellapsed:  -698.2574992179871\n",
      "total Loss:  -0.23046875\n",
      "total Loss:  -0.4287109375\n",
      "total Loss:  -0.1640625\n",
      "total Loss:  0.181640625\n",
      "total Loss:  -0.1630859375\n",
      "total Loss:  0.294921875\n",
      "total Loss:  -0.4140625\n",
      "total Loss:  0.2236328125\n",
      "total Loss:  -0.4697265625\n",
      "total Loss:  0.2080078125\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  15\n",
      "Time ellapsed:  -661.8443582057953\n",
      "total Loss:  -0.349609375\n",
      "total Loss:  0.1796875\n",
      "total Loss:  -0.28125\n",
      "total Loss:  -0.353515625\n",
      "total Loss:  0.3525390625\n",
      "total Loss:  -0.4931640625\n",
      "total Loss:  -0.58984375\n",
      "total Loss:  -0.3251953125\n",
      "total Loss:  -0.3505859375\n",
      "total Loss:  0.26953125\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  16\n",
      "Time ellapsed:  -698.6483361721039\n",
      "total Loss:  0.0556640625\n",
      "total Loss:  0.0986328125\n",
      "total Loss:  -0.3515625\n",
      "total Loss:  0.0126953125\n",
      "total Loss:  0.4892578125\n",
      "total Loss:  -0.25\n",
      "total Loss:  -0.2890625\n",
      "total Loss:  -0.1201171875\n",
      "total Loss:  -0.078125\n",
      "total Loss:  -0.4755859375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  17\n",
      "Time ellapsed:  -677.6490380764008\n",
      "total Loss:  0.775390625\n",
      "total Loss:  -0.9462890625\n",
      "total Loss:  -0.4990234375\n",
      "total Loss:  0.068359375\n",
      "total Loss:  -0.1298828125\n",
      "total Loss:  -0.375\n",
      "total Loss:  -0.169921875\n",
      "total Loss:  0.1494140625\n",
      "total Loss:  -0.294921875\n",
      "total Loss:  0.15234375\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  18\n",
      "Time ellapsed:  -679.0160548686981\n",
      "total Loss:  0.1796875\n",
      "total Loss:  0.2275390625\n",
      "total Loss:  -0.4091796875\n",
      "total Loss:  -0.287109375\n",
      "total Loss:  -0.3603515625\n",
      "total Loss:  0.0234375\n",
      "total Loss:  0.3095703125\n",
      "total Loss:  0.0595703125\n",
      "total Loss:  0.02734375\n",
      "total Loss:  0.38671875\n",
      "Skipping batch as its size 28 is smaller than 32.\n",
      "epoch:  19\n",
      "Time ellapsed:  -663.1396188735962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    startTime = time()\n",
    "    train_loss = 0.0  # Initialize train_loss for the epoch\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # print(batch)\n",
    "        # Unpack the batch into input features (X) and target labels (Y)\n",
    "        X, Y = batch\n",
    "        X = X.to(device)\n",
    "        Y = (Y[0].to(device), Y[1].to(device))\n",
    "        batch = [X, Y]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        predicted_params = out.view(-1, 2)  # Assuming your model output is of shape [batch_size, 2]\n",
    "        if len(Y) == 3:\n",
    "            print(Y)\n",
    "            continue\n",
    "        #actual_params = Y.view(-1, 2)  # Reshape Y to match the output\n",
    "        # print(X.shape)\n",
    "        batch_loss = custom_loss(predicted_params, Y, X, 32)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    print(\"epoch: \", epoch)\n",
    "    print(\"Time ellapsed: \", startTime - time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e01f17d-1dec-47e7-8ffe-8665e88aa3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_8_16_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d19d48f-df20-49b0-82fe-3654fb2e51f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelwithloss_8_16.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), checkpoint_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'modelwithloss_8_16_.pt')\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Model checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f739f2ca-85b2-48a0-839e-803122558354",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    print(\"not using GPU\")\n",
    "# might need to change to use multiple gpus\n",
    "device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = GNNRegressor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d1be164-8a93-4180-a706-cab95ba5904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded and is in evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69400/3561937107.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_8_16.pth'))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_8_16.pth'))\n",
    "print(\"Model has been loaded and is in evaluation mode.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1788a2-a8bd-4a80-bed5-26dbe276e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS\n",
    "validation_losses = []\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss to infinity\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start_time = time.time()\n",
    "    val_loss = 0.0  # Initialize val_loss for the epoch\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_loader:\n",
    "            # Unpack the batch into input features (X) and target labels (Y)\n",
    "            X, Y = batch\n",
    "            X = X.to(device)\n",
    "            Y = (Y[0].to(device), Y[1].to(device))\n",
    "            batch = [X, Y]\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch)\n",
    "            predicted_params = out.view(-1, 2)  # Assuming your model output is of shape [batch_size, 2]\n",
    "            \n",
    "            # Calculate loss\n",
    "            batch_loss = custom_loss(predicted_params, Y, X, 32)\n",
    "            val_loss += batch_loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    validation_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Model saved with validation loss {val_loss:.4f} at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd5e4912-04d7-4457-9bcd-c85ae1f4aceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_losses\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2db2def8-c52d-496a-a404-46abaa03e7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph1 = [[1, 12, 13, 8, 6, 4, 3], [11, 4, 9, 10, 0, 2, 3, 14, 12, 5], [8, 9, 1, 12, 10, 6, 14, 13], [14, 5, 12, 1, 10, 0], [10, 6, 1, 9, 5, 0], [3, 6, 8, 10, 14, 7, 4, 1, 12], [4, 5, 7, 13, 0, 2, 14], [9, 5, 6, 11], [5, 11, 2, 0, 12, 9], [1, 4, 7, 2, 14, 11, 13, 12, 8], [4, 1, 5, 3, 13, 2, 14], [1, 8, 14, 9, 13, 7, 12], [3, 13, 0, 2, 8, 1, 11, 9, 14, 5], [12, 0, 6, 14, 10, 9, 11, 2], [3, 11, 5, 9, 1, 13, 10, 2, 12, 6]]\n",
    "graph2 = [[12, 16, 19, 11, 2, 1], [18, 15, 10, 12, 16, 4, 17, 0], [4, 19, 7, 9, 3, 0], [16, 7, 11, 15, 4, 8, 6, 2, 10, 18, 9], [2, 11, 5, 3, 8, 19, 1, 18, 13], [14, 13, 18, 19, 12, 8, 4, 11, 7, 6], [7, 15, 10, 3, 8, 5, 16], [6, 16, 2, 3, 14, 15, 8, 5], [12, 14, 17, 19, 5, 3, 6, 4, 18, 16, 7], [11, 10, 15, 12, 16, 2, 14, 3, 19], [9, 1, 6, 15, 3, 14], [9, 15, 3, 4, 14, 5, 12, 0], [8, 14, 1, 5, 9, 0, 11], [5, 19, 18, 15, 4], [12, 5, 8, 7, 15, 11, 9, 10, 17], [6, 11, 1, 9, 3, 14, 10, 7, 18, 13], [17, 7, 3, 9, 1, 0, 8, 6], [16, 8, 1, 14], [1, 5, 13, 8, 15, 3, 4], [2, 13, 5, 8, 4, 0, 9]]\n",
    "graph3 = [[6, 7, 3, 12, 9, 10], [9, 11, 8, 12], [12, 8], [0, 9, 4], [6, 3, 9], [8, 10, 7], [0, 7, 4, 9], [0, 6, 5, 11], [5, 1, 2], [1, 3, 0, 6, 4], [11, 5, 0], [1, 10, 12, 7], [1, 0, 2, 11]]\n",
    "graph4 = [[11, 12, 6, 3], [3, 10, 4, 11], [8, 11, 13, 14, 12, 6], [11, 13, 1, 0], [12, 5, 9, 14, 1], [12, 10, 9, 4, 7], [8, 0, 2], [14, 5], [2, 6, 9, 14], [12, 5, 8, 4, 10, 13], [5, 9, 1, 13], [3, 2, 0, 13, 1, 14], [5, 9, 0, 4, 2], [3, 2, 11, 9, 10], [7, 8, 2, 4, 11]]\n",
    "graph5 = [[4, 8, 9, 1], [9, 0, 5], [5, 8, 3, 6], [2], [8, 0, 5, 9], [2, 4, 7, 1], [10, 2], [5], [2, 4, 0, 10, 9], [10, 1, 4, 0, 8], [9, 6, 8]]\n",
    "graph6 = [[13, 9, 7, 15, 6, 11, 2, 16, 1, 3, 4, 12], [12, 14, 4, 8, 10, 0, 17, 18, 3], [19, 14, 13, 9, 15, 11, 17, 7, 18, 10, 0, 16, 3], [11, 18, 17, 16, 15, 12, 19, 4, 10, 2, 0, 1], [7, 12, 11, 8, 17, 6, 1, 16, 13, 19, 3, 18, 0, 10], [9, 19, 6, 11, 14, 13, 17], [16, 19, 5, 4, 8, 12, 0, 17, 15], [4, 14, 9, 11, 8, 2, 0, 18, 12, 10, 19], [4, 14, 6, 7, 17, 1, 11, 9, 15, 16, 10], [5, 7, 17, 12, 0, 2, 16, 13, 15, 8], [16, 12, 17, 15, 2, 1, 3, 7, 8, 4], [4, 3, 15, 14, 5, 7, 2, 13, 18, 16, 0, 8], [4, 10, 1, 9, 16, 6, 3, 13, 14, 17, 7, 0], [0, 18, 15, 19, 2, 12, 9, 11, 4, 5, 17, 14, 16], [7, 17, 8, 16, 11, 2, 15, 19, 1, 5, 12, 13, 18], [11, 13, 17, 14, 18, 3, 2, 16, 10, 0, 9, 6, 8], [18, 10, 6, 14, 3, 12, 15, 4, 9, 11, 19, 0, 2, 8, 13], [14, 3, 4, 9, 15, 10, 2, 8, 6, 12, 13, 5, 1], [16, 13, 3, 15, 2, 11, 7, 1, 4, 14], [6, 2, 5, 13, 14, 3, 16, 4, 7]]\n",
    "graph7 = [[6, 1, 22], [20, 13, 0, 11, 6, 10, 3], [13, 18, 8, 9, 3, 17, 7, 14, 5, 16], [23, 13, 2, 1, 16], [5, 6, 13, 15, 19], [4, 16, 2, 11], [0, 15, 4, 1, 14, 10, 16, 12], [8, 18, 2, 22, 13], [10, 21, 7, 2, 19, 18, 11], [11, 2], [8, 23, 16, 11, 21, 18, 19, 1, 6], [1, 10, 9, 8, 5], [21, 17, 6], [2, 1, 19, 3, 4, 7, 15], [23, 6, 18, 22, 2], [22, 18, 6, 4, 17, 13], [10, 5, 6, 3, 2], [22, 12, 2, 15, 19], [15, 10, 21, 7, 2, 14, 23, 8], [13, 10, 21, 8, 4, 17], [1, 21], [10, 8, 12, 20, 18, 19, 22], [15, 17, 21, 14, 0, 7, 23], [14, 10, 3, 18, 22]]\n",
    "graph8 = [[18, 3, 17, 14, 11, 15, 7, 5, 8, 4, 19], [14, 19, 15, 5, 9, 17, 18, 13, 12, 16, 7, 3], [18, 14, 12, 3, 19, 15, 13, 9, 16, 10, 7, 4], [17, 16, 0, 14, 15, 13, 2, 12, 9, 6, 20, 5, 1], [15, 10, 6, 17, 9, 8, 0, 11, 12, 13, 14, 19, 2], [11, 1, 15, 12, 14, 18, 0, 7, 3, 19, 8, 17, 20], [19, 16, 4, 20, 12, 13, 3, 7, 17, 14], [13, 12, 20, 0, 16, 18, 5, 14, 6, 11, 2, 1, 17, 15, 19, 10], [14, 13, 17, 12, 4, 9, 0, 11, 16, 19, 15, 20, 5], [1, 11, 4, 3, 2, 8, 16, 12, 19, 18], [14, 4, 18, 13, 11, 12, 2, 19, 7], [17, 13, 5, 9, 0, 19, 18, 8, 4, 10, 7, 12, 14, 15], [7, 8, 2, 5, 6, 3, 17, 1, 19, 9, 14, 4, 10, 15, 18, 11], [7, 11, 14, 8, 3, 1, 2, 6, 10, 4, 17, 18, 19], [1, 17, 18, 8, 10, 3, 13, 16, 2, 0, 5, 7, 12, 6, 4, 11], [4, 20, 1, 19, 17, 3, 5, 2, 0, 8, 12, 7, 11], [3, 6, 14, 7, 2, 9, 8, 1], [3, 11, 14, 15, 0, 8, 1, 4, 12, 6, 7, 13, 5], [0, 14, 2, 20, 10, 19, 1, 11, 5, 7, 12, 13, 9], [1, 6, 15, 18, 2, 11, 12, 8, 10, 9, 5, 13, 4, 0, 7], [15, 18, 6, 7, 3, 8, 5]]\n",
    "graph9 = [[5, 2, 6, 1], [9, 2, 8, 0, 4], [0, 6, 1, 11, 9], [4, 9, 7], [3, 7, 10, 11, 1], [0, 10, 7, 11, 9], [10, 2, 0, 7, 9], [8, 4, 10, 5, 6, 3], [10, 7, 11, 1], [11, 1, 10, 5, 3, 2, 6], [5, 8, 6, 4, 7, 9], [9, 4, 8, 5, 2]]\n",
    "graph10 = [[17, 1, 7, 2, 12, 6, 4, 9, 18, 10, 5], [0, 17, 2, 12, 14, 4], [8, 23, 0, 1, 21, 16, 10, 7, 6], [22, 6, 15, 18, 11, 5, 17, 21, 23, 10, 4, 9, 16, 8, 20], [16, 15, 0, 9, 12, 17, 11, 3, 22, 7, 18, 1], [20, 7, 9, 15, 3, 8, 19, 23, 18, 0], [3, 19, 20, 12, 0, 23, 15, 2, 17], [16, 5, 0, 22, 9, 15, 14, 8, 2, 11, 4, 17, 20], [12, 14, 2, 11, 21, 7, 22, 19, 18, 5, 16, 20, 9, 3, 17], [7, 5, 15, 17, 4, 11, 3, 0, 8, 12], [16, 14, 13, 21, 2, 3, 23, 20, 0], [22, 8, 17, 20, 19, 3, 9, 4, 18, 16, 7, 14], [8, 20, 17, 0, 6, 21, 4, 1, 22, 9, 15], [15, 21, 14, 10], [8, 20, 7, 13, 18, 10, 15, 16, 23, 1, 11, 17], [13, 3, 19, 20, 7, 4, 5, 9, 14, 6, 12], [4, 7, 10, 23, 17, 2, 19, 14, 8, 3, 11, 18, 22], [0, 1, 22, 11, 12, 9, 3, 4, 16, 7, 14, 8, 19, 6, 18], [23, 21, 3, 14, 22, 8, 11, 20, 0, 16, 4, 5, 17], [23, 6, 15, 11, 8, 16, 5, 17, 22], [5, 12, 14, 15, 6, 11, 22, 8, 18, 10, 23, 7, 3], [13, 18, 12, 8, 10, 3, 2], [3, 17, 11, 7, 20, 8, 18, 4, 12, 16, 19], [19, 18, 2, 16, 3, 6, 10, 14, 5, 20]]\n",
    "def create_adjacency_matrix(adj_list, num_nodes):\n",
    "    adj_matrix = [[0] * num_nodes for _ in range(num_nodes)]  # Initialize with zeros\n",
    "    for node, neighbors in enumerate(adj_list):\n",
    "        for neighbor in neighbors:\n",
    "            adj_matrix[node][neighbor] = 1\n",
    "            adj_matrix[neighbor][node] = 1  # Assuming undirected graph\n",
    "    return adj_matrix\n",
    "graphs = [graph1, graph2, graph3, graph4, graph5, graph6, graph7, graph8, graph9, graph10]\n",
    "for i in range(len(graphs)):\n",
    "    graphs[i] = create_adjacency_matrix(graphs[i], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12bd38b7-c0f4-4ee8-a4ae-52a688926aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Predicted Gamma and Beta values: [[ 0.11482838 -0.06553309]]\n",
      "Graph 2 - Predicted Gamma and Beta values: [[ 0.07576785 -0.0340221 ]]\n",
      "Graph 3 - Predicted Gamma and Beta values: [[ 0.10506362 -0.03197668]]\n",
      "Graph 4 - Predicted Gamma and Beta values: [[ 0.12024899 -0.06295986]]\n",
      "Graph 5 - Predicted Gamma and Beta values: [[ 0.1158793 -0.0235382]]\n",
      "Graph 6 - Predicted Gamma and Beta values: [[ 0.21388616 -0.11889774]]\n",
      "Graph 7 - Predicted Gamma and Beta values: [[ 0.11343513 -0.04357992]]\n",
      "Graph 8 - Predicted Gamma and Beta values: [[ 0.154932  -0.1073247]]\n",
      "Graph 9 - Predicted Gamma and Beta values: [[ 0.11178534 -0.03763681]]\n",
      "Graph 10 - Predicted Gamma and Beta values: [[ 0.12161658 -0.10584046]]\n",
      "ellasped Time:  0.030094385147094727\n"
     ]
    }
   ],
   "source": [
    "graph_tensors = [torch.tensor(graph, dtype=torch.float32).unsqueeze(0).to(device) for graph in graphs]\n",
    "import time\n",
    "startTime = time.time()\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, graph_tensor in enumerate(graph_tensors):\n",
    "        # Predict gamma and beta values for each graph\n",
    "        predicted_gamma_beta = model([graph_tensor, None])  # None for Y since it's not needed during prediction\n",
    "\n",
    "        # Convert predictions to numpy for easier handling\n",
    "        predicted_gamma_beta = predicted_gamma_beta.cpu().numpy()\n",
    "\n",
    "        print(f\"Graph {i+1} - Predicted Gamma and Beta values: {predicted_gamma_beta}\")\n",
    "print(\"ellasped Time: \", time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660abc2c-6fe7-4586-bd73-150ff6bab76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
